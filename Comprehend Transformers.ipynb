{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feeb5d62-7f3a-4ea7-b6e7-e4441e66cd51",
   "metadata": {},
   "source": [
    "# 인코더 토큰화 층\n",
    "- 텍스트를 적절한 단위로 나누고 숫자로 변환\n",
    "- 자음,모음과 같은 가장 작은 단위로 하거나 음절, 단어로 토큰화 진행\n",
    "- 자음,모음의 경우 단어의 의미가 퇴색\n",
    "- 단어로 토큰화 할 경우 텍스트의 의미가 잘 유지 되지만 사전의 크기가 커짐\n",
    "- 최근은 ***서브 워드 토큰화*** 방식을 사용\n",
    "    - 자주 나오는 단어는 단어 단위 그대로 유지 하고 가끔 나오는 단어는 더 작은 단위로 나눔\n",
    "    - 예를 들어 \"대한민국\",\"안녕\"과 같은 단어는 유지, 외국어나 특수문자, 이모티콘은 작게 나눔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b294f-de84-47a8-92e8-37fee42a0396",
   "metadata": {},
   "source": [
    "#### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31289122-dade-4d55-a675-246bd6bce638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text_list :['나는', '이번에', '일본', '여행을', '다녀왔다.']\n"
     ]
    }
   ],
   "source": [
    "#띄어쓰기 단위 토큰화\n",
    "input_text=\"나는 이번에 일본 여행을 다녀왔다.\"\n",
    "input_text_list = input_text.split()\n",
    "print(f\"input_text_list :{input_text_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a7a442-b8ff-4f2e-bd7d-336769522601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 0, '이번에': 1, '일본': 2, '여행을': 3, '다녀왔다.': 4}\n",
      "{0: '나는', 1: '이번에', 2: '일본', 3: '여행을', 4: '다녀왔다.'}\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#토큰, 아이디 딕셔너리와 아이디, 토큰 딕셔너리 만들기\n",
    "str2idx={word:idx for idx,word in enumerate(input_text_list)}\n",
    "idx2str={idx:word for idx,word in enumerate(input_text_list)}\n",
    "print(str2idx)\n",
    "print(idx2str)\n",
    "#토큰을 토큰 아이디로 변환\n",
    "input_ids= [str2idx[word] for word in input_text_list]\n",
    "print(input_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d513b72-766e-4771-9f4f-7f9d7243ee04",
   "metadata": {},
   "source": [
    "#### 임베딩\n",
    "- 토큰화는 숫자일 뿐이므로 의미를 담을 수 없기 때문에 데이터를 의미를 담아 숫자 집합으로 변환 하는 **임베딩** 과정 필요\n",
    "- Pytorch의 nn.Embedding 클래스 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f436920-2b26-494b-84d9-130298ef427c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "embedding_dim=16\n",
    "embed_layer=nn.Embedding(len(str2idx),embedding_dim)\n",
    "\n",
    "input_embeddings= embed_layer (torch.tensor(input_ids))\n",
    "#1개의 문장 -> 1개 차원 설정\n",
    "input_embeddings=input_embeddings.unsqueeze(0)\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd09142-29af-4d5d-8120-8ce9a819b1ea",
   "metadata": {},
   "source": [
    "#### 위치 인코딩\n",
    "- RNN은 순차적으로 처리 하지만 Transformer는 모든 입력을 동시에 처리하는데, 그 과정에서 위치 정보가 사라지게 되니 텍스트에서는 위치가 중요한 정보이기 때문에 위치 인코딩을 해주어야 함\n",
    "- *Attention is All you need* 에서는 sin,cos 을 활용해 위치 정보를 입력했지만 이후 위치 인코딩도 위치에 따른 임베딩층을 추가해 학습 데이터를 통해 학습하는 방식을 많이 활용\n",
    "- 수식을 통한 위치 정보 추가 방식이나 임베딩으로 위치 정보 학습 방식 모두 결국 추론 수행 시험에서 입력 토큰의 위치에 따라 고정된 임베딩을 더해주기 때문에 <span  style='color:red'>**절대적 위치 인코딩**</span>이라고 함\n",
    "- 긴 텍스트를 추론하는 경우 성능이 떨어져 최근에는 <span  style='color:blue'> **상대적 위치 인코딩**</span> 을 많이 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe4347d-0fad-4322-9e80-ef5c2b097942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#absolute position encoding\n",
    "embedding_dim =16\n",
    "max_position=12\n",
    "embed_layer=nn.Embedding(len(str2idx),embedding_dim)\n",
    "position_embed_layer=nn.Embedding(max_position,embedding_dim)\n",
    "\n",
    "position_ids=torch.arange(len(input_ids),dtype=torch.long).unsqueeze(0)\n",
    "position_encodings= position_embed_layer(position_ids)\n",
    "token_embeddings= embed_layer(torch.tensor(input_ids))\n",
    "token_embeddings= token_embeddings.unsqueeze(0)\n",
    "input_embeddings=token_embeddings + position_encodings\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b44cdc-d656-4d89-a048-27e9f8fc20bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 어텐션\n",
    "- 앞 뒤 단어 없이'배'라는 단어가 있을때 ship인지 peer인지 알수 없음. 어텐션은 '나는 배 타고 다녀왔다' 타고,다녀왔다 에 주의(attention)해 '배'가 ship으로 해석한 것이다.\n",
    "- 단어 재해석의 어텐션 연산을 만드려면 **단어와 단어 사이의 관계를 계산해서 그 값에 따라 관련이 깊은 단어와 그렇지 않은 단어를 구분 해야함**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e552d-d25a-4bb9-9aa8-866fa74d73e8",
   "metadata": {},
   "source": [
    "### 쿼리(Query),키(Key),값(Value)\n",
    "- 우리가 입력하는 검색어 <span style=\"color:blue\">**쿼리**</span>\n",
    "- 쿼리와 관련있는지 계산하기 위해 문서가 가진 특징을 <span style=\"color:blue\">**키**</span>\n",
    "- 검색 엔진이 쿼리와 관련이 깊은 키를 가진 문서를 찾아 관련도 순으로 정렬해서 문서를 제공 할 때 <span style=\"color:blue\">**값**</span>\n",
    "- 예시 문장에서  <span style=\"color:blue\">** '배'가 쿼리**</span> , 키는 문장속의 각 단어\n",
    "- '배' 라는 쿼리로 ['나는','타고','다녀왔다'] 라는 키 묶음 에서 '타고','다녀왔다'가 적절히 섞여 높은 값이 된다면 단어 재해석을 모방할 수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2165a2-4e5e-426b-be1d-3470060b01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 쿼리,키,값 벡터 만드는 nn.Linear층\n",
    "head_dim = 16\n",
    "weight_q = nn.Linear(embedding_dim,head_dim)\n",
    "weight_k = nn.Linear(embedding_dim,head_dim)\n",
    "weight_v = nn.Linear(embedding_dim,head_dim)\n",
    "\n",
    "querys=weight_q(input_embeddings)\n",
    "keys = weight_k(input_embeddings)\n",
    "values = weight_v(input_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81877a15-550b-42be-9c33-0f38635300d2",
   "metadata": {},
   "source": [
    "#### 논문에서 처음 사용 된 스케일 점곱 방식 사용하기\n",
    "1. 쿼리와 키를 곱하고 분산을 줄이기 위해 임베딩 차원수의 제곱근으로 나눔\n",
    "2. 쿼리와 키를 곱한 scores를 합이 1이 되도록 softmax 취해 가중치로 변환\n",
    "3. 가중치와 값을 곱해 동일한 형태의 출력 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16afa20b-32ad-4936-a0c4-95d3f7e7f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "def compute_attention(querys,keys,values,is_causal=False):\n",
    "    dim_k= querys.size(-1)\n",
    "    scores=querys @ keys.transpose(-2,-1) / sqrt(dim_k) # 곱하기 위한 (5,16) Transpose\n",
    "    weights=F.softmax(scores,dim=-1)\n",
    "    return weights @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631fd3b7-5d5a-47e9-b944-6e605b38ea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 값 : torch.Size([1, 5, 16])\n",
      "어텐션 적용 : torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"원본 값 :\",input_embeddings.shape)\n",
    "after_attention_embeddings = compute_attention(querys,keys,values)\n",
    "print(\"어텐션 적용 :\",after_attention_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a496a2-e81b-4312-a88b-e50b7d655fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#현재 과정 AttentionHead 클래스 만들기\n",
    "class AttentionHead(nn.Module):\n",
    "    #쿼리,키,값 벡터 생성\n",
    "    def __init__(self,token_embed_dim,head_dim,is_causal=False):\n",
    "        super().__init__()\n",
    "        self.is_causal=is_causal\n",
    "        self.weight_q=nn.Linear(token_embed_dim,head_dim)\n",
    "        self.weight_k=nn.Linear(token_embed_dim,head_dim)\n",
    "        self.weight_v=nn.Linear(token_embed_dim,head_dim)\n",
    "    #compute_attention으로 어텐션 연산 수행\n",
    "    def forward(self,querys,keys,values):\n",
    "        outputs = compute_attention(\n",
    "            self.weight_q(querys),\n",
    "            self.weight_k(keys),\n",
    "            self.weight_v(values),\n",
    "            is_causal=self.is_causal\n",
    "        )\n",
    "        return outputs\n",
    "#embedding\n",
    "attention_head = AttentionHead(embedding_dim,embedding_dim)\n",
    "after_attention_embeddings = attention_head(input_embeddings,input_embeddings,input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603eb24-c922-4cb4-9545-e6b722586a20",
   "metadata": {},
   "source": [
    "### 멀티 헤드 어텐션\n",
    "토큰사이의 관계를 한 가지 측면에서 이해하는 것보다 여러 측면을 동시에 고려할 때 언어나 문장에 대한 이해도가 높아짐\n",
    "1. AttentionHead와 동일하지만 헤드의 수(n_head)만큼 연산을 수행 하기 위해 쿼리,키,값을 n_head개로 쪼갬\n",
    "2. 각각의 어텐션 계산\n",
    "3. 입력과 같은 형태로 다시 변환\n",
    "4. 마지막으로 선형층을 통과시키고 최종 결과 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625a289a-2a8b-46b5-bd53-cab10f811245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 헤드 수 4로 두고 멀티 헤드 어텐션 구현\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self,token_embed_dim,d_model,n_head,is_causal=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.is_causal = is_causal\n",
    "        self.weight_q=nn.Linear(token_embed_dim,head_dim)\n",
    "        self.weight_k=nn.Linear(token_embed_dim,head_dim)\n",
    "        self.weight_v=nn.Linear(token_embed_dim,head_dim)\n",
    "        self.concat_linear=nn.Linear(d_model,d_model)\n",
    "    def forward(self,querys,keys,values):\n",
    "        B,T,C = querys.size()\n",
    "        querys=self.weight_q(querys).view(B,T,self.n_head,C // self.n_head).transpose(1,2)\n",
    "        keys=self.weight_k(keys).view(B,T,self.n_head,C // self.n_head).transpose(1,2)\n",
    "        values=self.weight_v(values).view(B,T,self.n_head,C // self.n_head).transpose(1,2)\n",
    "        attention = compute_attention(querys,keys,values,self.is_causal)\n",
    "        output = attention.transpose(1,2).contiguous().view(B,T,C)\n",
    "        output = self.concat_linear(output)\n",
    "        return output\n",
    "n_head = 4\n",
    "mh_attention = MultiheadAttention(embedding_dim,embedding_dim,n_head)\n",
    "after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n",
    "after_attention_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14325b-f9b8-46dc-ac2f-9c1ab23c7233",
   "metadata": {},
   "source": [
    "# 정규화 & 피드 포워드\n",
    "- 정규화 : 딥러닝 모델에서 <span style='color: yellow'>**input 이 일정한 분포를 갖도록 만들어 학습이 안정적이고 빨라질 수 있도록 하는 기법**</span>\n",
    "    - 과거에는 batch input data 사이에 정규화를 수행하는 **배치 정규화**를 사용하지만\n",
    "    - 트랜스포머 아키텍쳐에서는 feature 차원에서 정규화를 수행하는 **층 정규화** 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155f8fc-b07b-487d-8cf7-5acd82ca96c2",
   "metadata": {},
   "source": [
    "\n",
    "### 층정규화 (Layer normalization)\n",
    "- 예를 들어 사람의 키는 140cm~200cm사이의 값을 가진다고 하면 mm로 단위로 변환 시 1400mm~2000mm의 단위로 변경된다. 단위 변경 시 데이터의 분포가 훨씬 넓어지고 모델이 데이터를 과대평가 할 가능성이 높아짐. 이러한 문제를 방지하기 위해 정규화를 해 **모든 데이터가 비슷한 range 와 distribution을 갖도록** 조정\n",
    "- 딥러닝 분야에서는 layer와 layer 사이에 정규화를 추가해 학습을 안정적으로 만드는 기법을 사용\n",
    "- 벡터 $X$ 를 정규화 한 $norm_x$ 는 벡터 $X$에서 $x$의 평균을 빼고 $x$의 표준편차로 나눠 평균이 0이고 표준 편차가 1인 분포를 갖게 됨\n",
    "$$norm_x = (X - 평균)/표준편차$$\n",
    "- 딥러닝 에서는 평균과 표준편차를 구할 데이터를 어떻게 묶는지에 따라 배치 정규화와 층 정규화로 구분\n",
    "- 배치 정규화의 경우 자연어 처리에서는 입력 문장 데이터 길이가 다양해 정규화 포함 데이터의 수가 제각각이라 정규화 효과 보장 X,\n",
    "- 층 정규화의 경우 각 토큰 임베딩의 평균과 표준편차를 이용해 정규화를 수행해 데이터의 수가 다르더라도 정규화 효과에 차이가 없음\n",
    "#### 트랜스포머에서의 **층 정규화**\n",
    "1. 피드포워드 층 이후 정규화\n",
    "    - 사후 정규화(post-norm) 라고 함\n",
    "2. 먼저 층 정규화 이후 어텐션과 피드포워드 층 통과\n",
    "    - 사전 정규화(pre-norm) 라고 함\n",
    "\n",
    ">On Layer Normalization in the Trnasformer Architecture 에 따르면 사전 정규화 방식이 학습이 더 안정적\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d38e96b-3c69-40d4-bad3-a12006cf068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "tensor([[[-1.7263,  0.6910, -2.2461, -0.4709, -1.7675, -0.5451, -0.7644,\n",
      "          -1.9052, -1.3890, -0.7695,  1.5643,  1.4007, -3.1769, -1.3379,\n",
      "           0.3272, -1.5821],\n",
      "         [-1.7494,  0.5201,  0.7756, -0.3975, -1.4791, -0.5822, -0.4589,\n",
      "          -0.6575,  0.1763, -0.1878,  1.7282,  0.6147,  0.4074,  1.8509,\n",
      "           0.1097,  0.2598],\n",
      "         [-1.0922, -0.7020, -0.9251, -0.4595, -1.4528, -1.7481, -1.0573,\n",
      "           0.1915, -2.4274, -1.2042,  0.0666, -0.7353, -0.4523, -0.7124,\n",
      "           1.8757, -0.9058],\n",
      "         [ 0.5254, -0.2783, -0.2952, -0.7015,  1.9352, -1.1526, -0.1530,\n",
      "          -1.7138, -4.4138, -0.3121,  1.6041,  3.2342,  0.8559,  0.4552,\n",
      "          -2.8547,  0.3139],\n",
      "         [ 0.1104, -1.2875, -1.9275,  1.2991,  0.3635, -1.9066, -1.4731,\n",
      "          -1.7893, -0.7840,  1.6650,  0.1627, -1.3894,  1.3903, -0.9919,\n",
      "           2.6928, -1.3044]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_dim)\n",
    "print(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79acc8e2-4b47-4a10-909a-d03ae713186f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 층 정규화 구현\n",
    "norm = nn.LayerNorm(embedding_dim)\n",
    "norm_x=norm(input_embeddings)\n",
    "norm_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54d24bed-a163-4c26-b4e6-a70aec262f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.7253e-08, -1.8626e-08,  7.4506e-09,  2.2352e-08, -1.4901e-08]]),\n",
       " tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]], grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_x.mean(dim=-1).data,norm_x.std(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fedf973-fcb9-4cd4-bab1-d91bb7ff96c2",
   "metadata": {},
   "source": [
    "\n",
    "### 피드포워드 층\n",
    "- 데이터의 특징을 학습하는 **완전 연결 층(fully connected layer)**\n",
    "- 멀티 헤드 어텐션이 단어사이의 관계 파악 역할이라면 **피드포워드 층은 입력 텍스트 전체를 이해**하는 역할\n",
    "- **선형 층,드롭아웃 층, 층 정규화,활성 함수로 구성**하고 임베딩의 차원을 동일하게 유지해야 쉽게 층을 쌓아 확장이 가능하기 때문에 **입력과 출력이 동일하도록**\n",
    "- 일반적으로 d_model 차원에서 d_model보다 2~3배 큰 dim_feedforward 차원으로 확장했다가 다시 d_model로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540f80e1-dbee-4dca-9395-ebe12b6fdbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#피드 포워드 층 구현\n",
    "class PreLayerNormFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,dim_feedforward,dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward) # 선형 층 1\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model) # 선형 층 2\n",
    "        self.dropout1 = nn.Dropout(dropout) # 드롭아웃 층 1 \n",
    "        self.dropout2 = nn.Dropout(dropout) # 드롭아웃 층 2\n",
    "        self.activation = nn.GELU() # 활성 함수\n",
    "        self.norm = nn.LayerNorm(d_model) #층 정규화\n",
    "    def forward(self,src):\n",
    "        x = self.norm(src)\n",
    "        x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e7b0a-6ea9-4aa7-a5a7-482278662b24",
   "metadata": {},
   "source": [
    "\n",
    "## 인코더 \n",
    "- 잔차 연결 (residual connection)이 있는 데 안정적으로 학습이 가능하도록 도와줌\n",
    "- $N_e$번 반복된다고 표시되어 있는데 트랜스포머 인코더는 인코더 블록을 반복해서 쌓아서 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a57907e6-ed78-4368-9eef-7a742c39811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadAttention(d_model, d_model, nhead) #멀티 헤드 어텐션 클래스\n",
    "        self.norm1 = nn.LayerNorm(d_model) # 층 정규화\n",
    "        self.dropout1 = nn.Dropout(dropout) # 드롭아웃\n",
    "        self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout) #피드 포워드\n",
    "    def forward(self,src):\n",
    "        norm_x = self.norm1(src) # 층 정규화 실시\n",
    "        attn_output = self.attn(norm_x, norm_x, norm_x) # 멀티 헤드 어텐션 연산 수행\n",
    "        x = src + self.dropout1(attn_output) #  잔차 연결을 위해 어텐션 결과에 드롭 아웃한 값과 입력값 합\n",
    "        x = self.feed_forward(x) # 피드포워드 연산\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7857767-e554-42e3-a3e1-1602abae2b33",
   "metadata": {},
   "source": [
    "#### $N_e$ 번 반복하게 코드 구현하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04714c55-8e3e-45ea-a70a-efd9c3e1d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $N_e$\n",
    "import copy\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "    def forward(self,src):\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631d216-0a36-4490-a96f-041ae7778777",
   "metadata": {},
   "source": [
    "## 디코더\n",
    "1. 인코더는 기본적인 멀티헤드 어텐션을 사용하지만, 디코더에서는 <span style='color:yellow'>**마스크 멀티헤드 어텐션 사용**</span>\n",
    "2. 디코더는 생성을 담당하는 부분으로, 사람이 글을 쓸때 앞단어 부터 작성하듯이 앞에서 생성한 토큰을 기반으로 다음 토큰을 생성함. (causal, auto-regressive 특징)\n",
    "\n",
    "#### 디코더 특징\n",
    "- 실제 텍스트 생성 시 디코더는 이전 까지 생성한 텍스트만 확인 할 수 있다. 그런데 학습할 때 인코더와 디코더 모두 완성된 텍스트를 입력받는다. 따라서 <span style='color : green'>**어텐션을 그대로 사용 시 미래 시점에 작성해야 하는 텍스트를 미리 확인하게 되는 문제가 생김**</span>\n",
    "- 이를 막기 위해 이전 생성 토큰만 확인 할 수 있도록 마스크 추가. 어텐션 코드에 is_causal을 추가해 디코더의 경우 True로 설정해 마스크 연산 추가\n",
    "- is_causal 이 참일 때 torch.ones로 모두 1인 행렬에 trill 함수를 취해 대각선 아래 부분만 1로 유지되고 나머지는 음의 무한대로 변경해 마스크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bc1b290-b9cc-4af1-a512-057c65353d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더 어텐션 연산(마스크 추가)\n",
    "def compute_attention(querys, keys, values, is_causal = False):\n",
    "    dim_k = querys.size(-1)\n",
    "    scores = querys @ keys.transpose(-2,-1) / sqrt(dim_k)\n",
    "    if is_causal:\n",
    "        query_length = querys.size(2)\n",
    "        key_length = keys.size(2)\n",
    "        temp_mask = torch.ones(query_length, key_length, dtype = torch.bool).trill(diagonal=0)\n",
    "        scores = scores.masked_fill(temp_mask == False, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim= -1)\n",
    "    return weights @ values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d211a8-4436-43c2-8185-581f6fb21e14",
   "metadata": {},
   "source": [
    "#### 크로스 어텐션\n",
    "- 예를 들어 영어를 한국어로 번역 할 때 영어 문장을 입력으로 받아 처리한 결과를 번역한 한국어를 생성하는 디코더가 받아서 활용.\n",
    "- 이때 쿼리는 디코더의 잠재 상태를 사용하고 key 와 value 는 인코더의 결과를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c49adae-e7d5-4304-b4f7-28c49cc4f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#크로스 어텐션이 포함된 디코더 층\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, d_model, nhead)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, d_model, nhead)\n",
    "        self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, tgt, encoder_output, is_causal = True):\n",
    "        #셀프 어텐션\n",
    "        x = self.norm1(tgt)\n",
    "        x = x + self.dropout1(self.self_attn(x,x,x))\n",
    "        #크로스 어텐션\n",
    "        x = self.norm2(x)\n",
    "        x=  x + self.dropout2(self.multihead_attn(x, encoder_output, encoder_output, is_causal = is_causal))\n",
    "        #피드 포워드\n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de47b0a-ed2b-42b0-ba9d-7d26b07a636c",
   "metadata": {},
   "source": [
    "#### $N$번 반복하게 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6feb4ce7-ffda-47b5-a4c2-2d6f7957d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "    def forward(self,tgt, src):\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output = mod(tgt, src)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894542c-c066-4388-a3c8-3b98ef05cf51",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>모델 그룹</th>\n",
    "            <th>대표</th>\n",
    "            <th>장점</th>\n",
    "            <th>단점</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>인코더(NLU)</td>\n",
    "            <td>BERT</td>\n",
    "            <td>양방향 이해를 통해 자연어 이해에서 디코더 모델 대비 높은 성능</td>\n",
    "            <td>자연어 생성 작업(NLG)에 부적합 하고 컨텍스트의 길이가 제한적</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>디코더(NLG)</td>\n",
    "            <td>GPT</td>\n",
    "            <td>생성작업에서 뛰어난 성능,긴 컨텍스트 길이에 대해서 성능이 좋음</td>\n",
    "            <td>단방향 방식으로 자연어 이해 작업(NLU)에서 비교적 성능이 낮음</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>인코더-디코더</td>\n",
    "            <td>BART,T5</td>\n",
    "            <td>생성과 이해 모두 뛰어남, 이해 작업에서 양방향 방식을 사용</td>\n",
    "            <td>복잡함, 학습에 많은 컴퓨팅 자원 필요</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f511bd-a8e6-4d2f-b83e-9c2f500c3ef6",
   "metadata": {},
   "source": [
    "##### 인코더를 활용한 BERT\n",
    "- 양방향 문맥을 모두 활용해 텍스트를 이해한다.\n",
    "- 입력 토큰의 일부를 마스크 토큰으로 대체하고 마스크 토큰을 맞추는 마스크 언어 모델링 과제를 통해 사전 학습\n",
    "- 다운스트림 과제에 따라 미세 조정해 사용\n",
    "- 텍스트 분류뿐만 아니라 토큰 분류, 질문 답변, 자연어 추론 등 다양한 자연어 이해 작업에서 훌륭한 성능\n",
    "##### 디코더를 활용한 GPT\n",
    "- 생성 작업의 경우 입력 토큰이나 이전까지 생성한 토큰만을 문맥으로 활용하는 **인과적 언어 모델링**을 사용하기 떄문에 단뱡향 방식\n",
    "- 다음 토큰을 예측하는 방식으로 사전 학습 수행\n",
    "##### 인코더와 디코더를 모두 사용하는 BART,T5\n",
    "- BART는 모델 사전 학습을 위해 입력에 노이즈를 추가하고 노이즈가 제거된 결과를 생성하는 과제 수행\n",
    "- 인코더 부분이 양방향 추론이 가능하다는 점이 BART의 특징이며 더 자유로운 변형 추가가 가능하다는 점에서 BERT와 차이가 있음\n",
    "- T5는 입력의 시작에 과제 종류를 지정해 하나의 모델에서 다양한 동작을 하도록 학습시킨 점이 특징\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e855653-b44a-431c-b54c-3e84a83a109b",
   "metadata": {},
   "source": [
    "# 사전 학습 매커니즘\n",
    "### 인과적 언어 모델링\n",
    "- 문장의 시작부터 끝까지 순차적으로 단어를 예측하는 방식. 이전에 등장한 단어를 바탕으로 다음에 등장할 단어를 예측\n",
    "- GPT 같은 생성 트랜스포머 모델에서는 인과적 언어 모델링을 핵심적인 학습 방법으로 사용\n",
    "### 마스크 언어 모델링\n",
    "- 입력 단어 일부를 마스크 처리하고 그 단어를 맞추는 작업으로 모델을 학습시킨다.\n",
    "- 인과적 언어 모델링은 앞에서부터 뒤로 순차적으로 생성하는데, 이 방식은 지금까지 생성한 문맥만 활용할 수 있다는 한계가 있음.\n",
    "- \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cba475-11cc-4618-af16-79a05019751a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
