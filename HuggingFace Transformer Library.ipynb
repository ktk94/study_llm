{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c068b291-3363-4b07-8fb7-9a38a0da244c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (0.33.0)\n",
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/2a/7c/6daed4be624f903643edd1b91670ce321a4cf08afd2b96ac368461742e89/accelerate-1.0.0-py3-none-any.whl.metadata\n",
      "  Using cached accelerate-1.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached accelerate-1.0.0-py3-none-any.whl (330 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.33.0\n",
      "    Uninstalling accelerate-0.33.0:\n",
      "      Successfully uninstalled accelerate-0.33.0\n",
      "Successfully installed accelerate-1.0.0\n",
      "Requirement already satisfied: transformers in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (4.43.3)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/f9/9d/030cc1b3e88172967e22ee1d012e0d5e0384eb70d2a098d1669d549aea29/transformers-4.45.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/44.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.4 kB 217.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 363.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.21,>=0.20 from https://files.pythonhosted.org/packages/f1/95/f1b56f4b1fbd54bd7f170aa64258d0650500e9f45de217ffe4d4663809b6/tokenizers-0.20.1-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.20.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ktaek\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.6/9.9 MB 18.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.3/9.9 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 32.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.1/9.9 MB 35.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.1/9.9 MB 37.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 39.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 35.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 30.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.1-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.4/2.4 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 30.1 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.43.3\n",
      "    Uninstalling transformers-4.43.3:\n",
      "      Successfully uninstalled transformers-4.43.3\n",
      "Successfully installed tokenizers-0.20.1 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "#ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U 해결\n",
    "! pip install -U accelerate\n",
    "! pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2d594-7184-449a-9beb-4ed1a8bd586e",
   "metadata": {},
   "source": [
    "#### 허깅페이스 트랜스포머 활용 라이브러리사용 시 \n",
    "!pip install transformers==4.40.1 datasets==2.19.0 huggingface_hub==0.23.0 -qq\n",
    "\n",
    "- huggingface transformer 라이브러리 사용하면 허깅페이스 모델 허브의 모델을 쉽게 불러와 사용가능\n",
    "- **꼭 알아야 하는 사실은 huggingface는 모델을 body와 head로 구분한다. 이유는 같은 바디를 사용하면서 다른 작업에 사용할 수 있도록 만들기 위해서다**\n",
    "- 모델의 바디만 불러올 수 있고, 헤드와 함께 불러올 수도 있다.\n",
    "- 허깅페이스 모델을 저장할 때 config.json 파일이 함께 저장되는데,해당 설정 파일에는 **모델의 종류,설정 파라미터,어휘 사전 크기,토크나이저 클래스 등**이 저장된다. 이를 참고해서 AutoModel,AutoTokenizer 는 적절한 모델과 토크나이저를 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "927e38e3-dcc7-407c-9057-f59f81fdb615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## 모델 바디 불러오기\n",
    "from transformers import AutoModel\n",
    "model_id = 'klue/roberta-base'\n",
    "model= AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e082db-ae4d-4b80-9aef-5a32b9fd9da8",
   "metadata": {},
   "source": [
    "AutoModelForSequenceClassification은 텍스트 시퀀스 분류를 위한 헤드가 포함된 모델을 불러올 때 사용하는 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07104f58-5dd5-4f51-99bf-1d8c8ef918e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 텍스트 분류 헤드가 붙은 모델 불러오기\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d33cd-de13-4932-8bd5-03955a18c733",
   "metadata": {},
   "source": [
    "### 허깅 페이스로 불러온 모델의 저장 경로\n",
    "~/.cache/huggingface/transformers/\n",
    "\n",
    "- AutoConfig 를 활용하면 모델의 특성을 확인 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a73dc0a-2acd-4de6-82d1-9f9369009797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"klue/roberta-base\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789e269-ed5a-4e71-8988-dda9ad1cb9bf",
   "metadata": {},
   "source": [
    "### 토크나이저 \n",
    "- 토크나이저는 텍스트를 토큰 단위로 나누고 각 토큰을 대응 하는 토큰 아이디로 변환하거나 필요의 경우 특수 토큰을 추가하는 역할도 한다.\n",
    "- AutoTokenizer 클래스를 이용해 토크나이저를 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887cecc9-1c5d-42e7-bd1f-fed504ff5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c09651-fde1-461e-ba8a-7b9010c018cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 4835, 2088, 1793, 2855, 15129, 2200, 15070, 2205, 9253, 5177, 1793, 2855, 4140, 4008, 2069, 3605, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나누', '##고', '토', '##큰', '아이디', '##로', '변환', '##하', '##거나', '특수', '토', '##큰', '추가', '역할', '##을', '한다', '[SEP]']\n",
      "[CLS] 토크나이저는 텍스트를 토큰 단위로 나누고 토큰 아이디로 변환하거나 특수 토큰 추가 역할을 한다 [SEP]\n",
      "토크나이저는 텍스트를 토큰 단위로 나누고 토큰 아이디로 변환하거나 특수 토큰 추가 역할을 한다\n"
     ]
    }
   ],
   "source": [
    "#토크나이저 사용해보기\n",
    "tokenized = tokenizer('토크나이저는 텍스트를 토큰 단위로 나누고 토큰 아이디로 변환하거나 특수 토큰 추가 역할을 한다')\n",
    "print(tokenized)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "\n",
    "print(tokenizer.decode(tokenized['input_ids']))\n",
    "\n",
    "print(tokenizer.decode(tokenized['input_ids'],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdc649-77c4-4d3b-af63-69950c472db3",
   "metadata": {},
   "source": [
    "- input_ids는 토큰화 했을 때 각 토큰이 토크나이저 사전의 몇번째 항목인지 나타낸다.\n",
    "- 첫번째 항목은 0 인데 이는 [CLS] 토큰으로 대응된다.\n",
    "- attention_mask가 1이면 padding token이 아닌 실제 토큰을 의미함.\n",
    "- token_type_ids가 0 이면 일반적으로 첫번째 문장을 의미한다.\n",
    "- 토큰 아이디를 다시 텍스트로 돌리고 싶다면 토크나이저의 decode 메서드를 사용하고, 만약 [CLS] 나 [SEP] 과 같은 특수 토큰을 제외하고싶으면 skip_special_tokens 인자를 True로 설정하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0cac48-2f32-4968-ae03-effca85399dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 1656, 2517, 3135, 6265, 2], [0, 864, 2517, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 1656, 2517, 3135, 6265, 2, 864, 2517, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# 여러개 문장 처리하기\n",
    "print(tokenizer(['첫번째 문장','두번째 문장']))\n",
    "# 2개의 문장이 한번에 모델에 입력되길 원하면 리스트로 묶어야\n",
    "print(tokenizer([['첫번째 문장','두번째 문장']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85358d4a-98fd-4df8-90bc-0ec82aa4821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] 첫번째 문장 [SEP]', '[CLS] 두번째 문장 [SEP]']\n",
      "['[CLS] 첫번째 문장 [SEP] 두번째 문장 [SEP]']\n"
     ]
    }
   ],
   "source": [
    "# batch_decode() 메서드 사용 시 input_ids 토큰 아이디를 문자열로 복원 할 수 있다.\n",
    "# 기본적으로 토큰화시 CLS 와 SEP으로 나뉘는데 2개의 문장 토큰화 시 SEP으로 두 문장을 구분할 수 있다.\n",
    "first_tokenized_result = tokenizer(['첫번째 문장','두번째 문장'])['input_ids']\n",
    "print(tokenizer.batch_decode(first_tokenized_result))\n",
    "\n",
    "second_tokenized_result = tokenizer([['첫번째 문장','두번째 문장']])['input_ids']\n",
    "print(tokenizer.batch_decode(second_tokenized_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9616b9-96cd-425f-8a42-cbfea0a2ae61",
   "metadata": {},
   "source": [
    "- 토큰화 결과 중 token_type_ids는 문장을 구분하는 역할. BERT는 학습 시 2개의 문장이 서로 이어지는지 맞추는 NSP(Next Sentence Prediction)작업을 활용하는데 이를 위해 문장을 구분하는 token type id를 만들었다.\n",
    "- RoBERTa 계열 모델의 경우 NSP작업을 학습 과정에서 제거 했기 때문에 문장 토큰 구분이 필요없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119a860b-f113-47d1-9cfd-defdd9281880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 1656, 2517, 3135, 6265, 3, 864, 2517, 3135, 2346, 2121, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 1656, 2517, 3135, 6265, 2, 864, 2517, 3135, 2346, 2121, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "print(bert_tokenizer([['첫번째 문장','두번째문장']]))\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "print(roberta_tokenizer([['첫번째 문장','두번째문장']]))\n",
    "\n",
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(en_roberta_tokenizer([['first sentence','second sentence']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e9797-8201-4af6-8d9b-d6e2aca5bdc8",
   "metadata": {},
   "source": [
    "- Attention_mask는 해당 토큰이 패딩인지 정보를 담고 있다.\n",
    "- 패딩은 모델에 입력하는 토큰 아이디의 길이를 맞추기 위해 추가하는 특수 토큰이다\n",
    "- padding 인자에 longest를 추가하면 긴 문장에 맞춰 패딩 토큰을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c6978c6-1546-4780-b7f9-f286c9456233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1599, 2073, 6265, 2, 1, 1], [0, 5370, 831, 646, 6265, 28674, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attention_mask 확인\n",
    "tokenizer(['짧은 문장','이건 더 긴 문장이다'],padding = 'longest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d7e3b-4bed-4f33-b3e9-b6d6217feac1",
   "metadata": {},
   "source": [
    "## DataSets 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e99a2d-9235-4e09-8c3a-2e05e6e441aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 17554\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 5841\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "klue_mrc_dataset = load_dataset('klue','mrc')\n",
    "klue_mrc_dataset\n",
    "\n",
    "#만약에 train(test,validataion)데이터만 필요하다고 하면 아래와 같은 코드\n",
    "#only_train = load_dataset('klue','mrc','split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34167c3a-0ca6-4f24-a1f7-a035984dd124",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1379864946.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    df = pd.DataFrame({'A'}:[1,2,3])\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 로컬의 csv 데이터 활용하기\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv',data_files = 'file.csv')\n",
    "#딕셔너리 사용\n",
    "from datasets import Dataset\n",
    "my_dict = {'A' :[1,2,3]}\n",
    "dataset = Dataset.from_dict(my_dict)\n",
    "#판다스 활용\n",
    "from datasets import Dataset\n",
    "df = pd.DataFrame({'A':[1,2,3]})\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f986f5c-afe4-4be9-95a5-584a531e2e52",
   "metadata": {},
   "source": [
    "## 모델 학습 하기\n",
    "\n",
    "- 한국어 기사 제목으로 기사의 카테고리 분류하는 모델 만들기\n",
    "- HuggingFace Transformer에서는 학습 과정을 추상화한 Trainer API를 제공한다. 이를 사용하면 학습을 간편하게 할 수 있지만,내부에서 어떤 과정을 거치는지 알기 어렵다는 단점도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d66b95f-64ef-4b21-aead-461236a118f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 셋 준비\n",
    "klue_tc_train = load_dataset('klue','ynat',split = 'train')\n",
    "klue_tc_eval = load_dataset('klue','ynat',split = 'validation')\n",
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50f616-beb2-4e0f-bd0a-e6a7a0937289",
   "metadata": {},
   "source": [
    "guid : 데이터의 고유 id\n",
    "\n",
    "\n",
    "title : 뉴스 제목\n",
    "\n",
    "label : 속한 카테고리 ID\n",
    "\n",
    "url : 뉴스 링크\n",
    "\n",
    "date : 뉴스 입력 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c7a45a0-5231-48ce-a109-e219932420a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train.features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13153af2-d815-45b7-adc7-180a42defb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'label_str': '생활문화'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불필요 행 제거\n",
    "klue_tc_train = klue_tc_train.remove_columns(['guid','url','date'])\n",
    "klue_tc_eval = klue_tc_eval.remove_columns(['guid','url','date'])\n",
    "# 레이블 카테고리 열 추가\n",
    "klue_tc_label = klue_tc_train.features['label']\n",
    "def make_label(batch):\n",
    "    batch['label_str'] = klue_tc_label.int2str(batch['label'])\n",
    "    return batch\n",
    "klue_tc_train = klue_tc_train.map(make_label,batched=True,batch_size = 1000)\n",
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4357a7a-164d-4ac0-a647-75ab3bf6ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10,000개만 가지고 해보자\n",
    "train_dataset = klue_tc_train.train_test_split(test_size=10000 , shuffle = True)['test']\n",
    "dataset = klue_tc_eval.train_test_split(test_size=1000, shuffle = True)\n",
    "test_dataset = dataset['test']\n",
    "valid_dataset = dataset['train'].train_test_split(test_size=1000,shuffle=True)['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137ac87-e46b-4f81-a472-29ad89fe9b19",
   "metadata": {},
   "source": [
    "\n",
    "## Trainer API 사용하기\n",
    "\n",
    "- 학습에 필요한 다양한 기능(데이터로더,로깅,평가,저장)을 TrainingArguments만으로 쉽게 활용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afafb520-3ec2-4fb1-a870-028044c4d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3668b5283bac409cb1d213870c26fd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e710dcc75f7462e966693652c2d57f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20914651ece34d9a859864205b7b0a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/1250 02:46 < 19:09:52, 0.02 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#학습 진행하기\u001b[39;00m\n\u001b[0;32m     29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     31\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     36\u001b[0m )\n\u001b[1;32m---> 37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     39\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2053\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2054\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2055\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2056\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2057\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2246\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer)\n",
    "def tokenizer_function(examples):\n",
    "    return tokenizer(examples['title'],padding='max_length',truncation=True)\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels = len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "train_dataset = train_dataset.map(tokenizer_function,batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenizer_function,batched=True)\n",
    "test_dataset = test_dataset.map(tokenizer_function,batched=True)\n",
    "\n",
    "#학습이 끝날 때마다 검증 데이터 평가가 이루어 지도록 evaluation_strategy = epoch으로 설정\n",
    "#학습이 잘 이루어 지는지 확인할 때 사용할 평가지표 정의. 예측 결과인 eval_pred를 입려긍로 받아 예측 결과 중 가장 큰 값을 갖는 클래스를 np.argmax로 뽑아 predictions 변수에 저장하고 labels와 같은 값을 갖는 결과 비율을 정확도로 반환\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./result\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    learning_rate = 5e-5,\n",
    "    push_to_hub = False)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    prediction = np.argmax(logits,axis = -1)\n",
    "    return {'Accuracy' : (prediction==labels).mean()}\n",
    "\n",
    "#학습 진행하기\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab56f3d-d895-436e-9f34-94c1f89bf987",
   "metadata": {},
   "source": [
    "## Trainer API 없이 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe279c-c389-48ab-87ff-33183c9a4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 모델과 토크나이저 준비\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# title 토큰화\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['title'],padding='max_length',truncation=True)\n",
    "#모델,토크나이저 불러오기\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id,num_lables = len(train_dataset.features['lable'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0855c-6501-4332-88eb-dc8e962e490a",
   "metadata": {},
   "source": [
    "1. model.train()으로 모델을 학습모드로 전환\n",
    "2. 배치 데이터를 가지고와서 모델에 입력으로 전달(input_ids,attention_mask,정답레이블 labels)\n",
    "3. 레이블과의 차이를 통해 계산된 loss값으로 역전파 수행하고, 옵티마이저의 step 호출로 역전파 결과를 바탕으로 모델 업데이트\n",
    "4. total_loss의 경우 학습이 잘되고 있는지 집게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70226b8-057f-4fd2-a8aa-5c92b808eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 데이터 준비\n",
    "def make_dataloader(dataset, batch_size, shuffle=True):\n",
    "    dataset = dataset.map(tokenize_function,batched=True).with_format('torch')\n",
    "    dataset = dataset.rename_column('label','labels')\n",
    "    dataset = dataset.remove_columns(column_names = ['title'])\n",
    "    return DataLoader(dataset,batch_size = batch_size, shuffle = shuffle)\n",
    "train_dataloader = make_dataloader(train_dataset,batch_size = 8, shuffle=True)\n",
    "valid_dataloader = make_dataloader(valid_dataset,batch_size = 8, shuffle=False)\n",
    "test_dataloader = make_dataloader(test_dataset,batch_size = 8, shuffle=False)\n",
    "\n",
    "#학습 함수\n",
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['lables'].to(device)\n",
    "        outputs = model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss +=loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664187c9-2208-4c3b-9fca-c2ce4304cf6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516bc3bb-2ce6-4a8b-9895-e6cc2e54c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "def evaluate(model,data_loader):\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    predictions=[]\n",
    "    true_labels=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['lables'].to(device)\n",
    "            outputs = model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits,dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss/len(data_loader)\n",
    "    accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "    return avg_loss,accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
